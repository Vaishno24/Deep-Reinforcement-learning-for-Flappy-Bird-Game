{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b58ad126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d52e9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent(object):\n",
    "\n",
    "\tdef __init__(self,train):\n",
    "\t\tself.train = train\n",
    "\t\tself.episode = 0\n",
    "\t\tself.discount_factor = 0.95\n",
    "\t\tself.learning_rate = 0.7\n",
    "\t\tself.previous_state = [96,47,0]\n",
    "\t\tself.previous_action = 0\n",
    "\t\tself.moves = []\n",
    "\t\tself.scores = []\n",
    "\t\tself.max_score = 0\n",
    "\t\tself.xdim = 130\n",
    "\t\tself.ydim = 130\n",
    "\t\tself.vdim = 20\n",
    "\t\t# initialize matrix to store qvalues\n",
    "\t\tself.qvalues = np.zeros((self.xdim, self.ydim, self.vdim, 2))\n",
    "\t\tself.initialize_model()\n",
    "\n",
    "\tdef initialize_model(self):\n",
    "\t\t# load episode, qvalues from already existing qvalues.txt\n",
    "\t\tif os.path.exists(\"qvalues.txt\"):\n",
    "\t\t\tqfile = open(\"qvalues.txt\",\"r\")\n",
    "\t\t\tline = qfile.readline()\n",
    "\t\t\tif self.train:\n",
    "\t\t\t\tself.episode = int(line)\n",
    "\t\t\tline = qfile.readline()\n",
    "\t\t\twhile len(line) != 0:\n",
    "\t\t\t\tstate = line.split(',')\n",
    "\t\t\t\tself.qvalues[int(state[0]),int(state[1]),int(state[2]),int(state[3])] = float(state[4])\n",
    "\t\t\t\tline = qfile.readline()\n",
    "\t\t\tqfile.close()\n",
    "\t\t\n",
    "\tdef act(self, xdist, ydist, vely):\n",
    "\t\t# store the transition from previous state to current state\n",
    "\t\tif self.train:\n",
    "\t\t\tstate = [xdist,ydist,vely]\n",
    "\t\t\tself.moves.append([self.previous_state,self.previous_action,state,0])\n",
    "\t\t\tself.previous_state = state\n",
    "\n",
    "\t\t# get action with max qvalue for current state\n",
    "\t\tif self.qvalues[xdist,ydist,vely][0] >= self.qvalues[xdist,ydist,vely][1]:\n",
    "\t\t\tself.previous_action = 0\n",
    "\t\telse:\n",
    "\t\t\tself.previous_action = 1\n",
    "\t\treturn self.previous_action\n",
    "\n",
    "\tdef record(self,reward):\n",
    "\t\t# set reward to the last transition\n",
    "\t\tself.moves[-1][3] = reward\n",
    "\n",
    "\tdef update_qvalues(self, score):\n",
    "\t\tself.episode += 1\n",
    "\t\tself.max_score = max(self.max_score, score)\n",
    "\t\tprint(\"Episode: \" + str(self.episode) + \" Score: \" + str(score) + \" Max Score: \" + str(self.max_score))\n",
    "\t\tself.scores.append(score)\n",
    "\t\t\n",
    "\t\tif self.train:\n",
    "\t\t\thistory = list(reversed(self.moves))\t\t\t\n",
    "\t\t\tfirst = True\n",
    "\t\t\tsecond = True\n",
    "\t\t\tjump = True\n",
    "\t\t\tif history[0][1] < 69:\n",
    "\t\t\t\tjump = False\n",
    "\t\t\tfor move in history:\n",
    "\t\t\t\t[x,y,v] = move[0]\n",
    "\t\t\t\taction = move[1]\n",
    "\t\t\t\t[x1,y1,z1] = move[2]\n",
    "\t\t\t\treward = move[3]\n",
    "\t\t\t\t# penalize last 2 states before crash\n",
    "\t\t\t\tif first or second:\n",
    "\t\t\t\t\treward = -1000000\n",
    "\t\t\t\t\tif first:\n",
    "\t\t\t\t\t\tfirst = False\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tsecond = False\n",
    "\t\t\t\t# penalize last jump before crash\n",
    "\t\t\t\tif jump and action:\n",
    "\t\t\t\t\treward = -1000000\n",
    "\t\t\t\t\tjump = False\n",
    "\t\t\t\tself.qvalues[x,y,v,action] = (1- self.learning_rate) * (self.qvalues[x,y,v,action]) + (self.learning_rate) * ( reward + (self.discount_factor)*max(self.qvalues[x1,y1,z1,0],self.qvalues[x1,y1,z1,1]))\n",
    "\t\t\tself.moves = []\n",
    "\n",
    "\tdef save_model(self):\n",
    "\t\t# write the episode, qvalues to qvalues.txt\n",
    "\t\tdata = str(self.episode) + \"\\n\"\n",
    "\t\tfor x in range(self.xdim):\n",
    "\t\t\tfor y in range(self.ydim):\n",
    "\t\t\t\tfor v in range(self.vdim):\n",
    "\t\t\t\t\tfor a in range(2):\n",
    "\t\t\t\t\t\tdata += str(x) + \", \" + str(y) + \", \" + str(v) + \", \" + str(a) + \", \" + str(self.qvalues[x,y,v,a]) + \"\\n\"\n",
    "\t\tqfile = open(\"qvalues.txt\",\"w\")\n",
    "\t\tqfile.write(data)\n",
    "\t\tqfile.close()\n",
    "\t\t\n",
    "\t\t# append the scores to scores.txt\n",
    "\t\tdata1 = ''\n",
    "\t\tfor i in range(len(self.scores)):\n",
    "\t\t\tdata1 += str(self.scores[i]) + \"\\n\"\n",
    "\t\tsfile = open(\"scores.txt\",\"a+\")\n",
    "\t\tsfile.write(data1)\n",
    "\t\tsfile.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "518795b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgentGreedy(object):\n",
    "\n",
    "\tdef __init__(self,train):\n",
    "\t\tself.train = train\n",
    "\t\tself.episode = 0\n",
    "\t\tself.discount_factor = 0.95\n",
    "\t\tself.learning_rate = 0.7\n",
    "\t\tself.previous_state = [96,47,0]\n",
    "\t\tself.previous_action = 0\n",
    "\t\tself.epsilon = 0.1\n",
    "\t\tself.final_epsilon = 0.0\n",
    "\t\tself.epsilon_decay = 0.00001\n",
    "\t\tself.max_score = 0\n",
    "\t\tself.xdim = 130\n",
    "\t\tself.ydim = 130\n",
    "\t\tself.vdim = 20\n",
    "\t\tself.moves = []\n",
    "\t\tself.scores = []\n",
    "\t\t# initialize matrix to store qvalues\n",
    "\t\tself.qvalues = np.zeros((self.xdim, self.ydim, self.vdim, 2))\n",
    "\t\tself.initialize_model()\n",
    "\n",
    "\tdef initialize_model(self):\n",
    "\t\t# load episode, qvalues from already existing qvalues.txt\n",
    "\t\tif os.path.exists(\"qvalues_greedy.txt\"):\n",
    "\t\t\tqfile = open(\"qvalues_greedy.txt\",\"r\")\n",
    "\t\t\tline = qfile.readline()\n",
    "\t\t\tif self.train:\n",
    "\t\t\t\t[self.episode,self.epsilon] = [int(line.split(',')[0]), float(line.split(',')[1])]\n",
    "\t\t\tline = qfile.readline()\n",
    "\t\t\twhile len(line) != 0:\n",
    "\t\t\t\tstate = line.split(',')\n",
    "\t\t\t\tself.qvalues[int(state[0]),int(state[1]),int(state[2]),int(state[3])] = float(state[4])\n",
    "\t\t\t\tline = qfile.readline()\n",
    "\t\t\tqfile.close()\n",
    "\t\t\n",
    "\tdef act(self, xdist, ydist, vely):\n",
    "\t\t# store the transition from previous state to current state\n",
    "\t\tif self.train:\n",
    "\t\t\tstate = [xdist,ydist,vely]\n",
    "\t\t\tself.moves.append([self.previous_state,self.previous_action,state,0])\n",
    "\t\t\tself.previous_state = state\n",
    "\n",
    "\t\t\t# get an action epsilon greedy policy\n",
    "\t\t\tif random.random() <= self.epsilon:\n",
    "\t\t\t\tself.previous_action = random.randrange(2)\n",
    "\t\t\telif self.qvalues[xdist,ydist,vely][0] >= self.qvalues[xdist,ydist,vely][1]:\n",
    "\t\t\t\tself.previous_action = 0\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.previous_action = 1\n",
    "\t\telse:\n",
    "\t\t\tif self.qvalues[xdist,ydist,vely][0] >= self.qvalues[xdist,ydist,vely][1]:\n",
    "\t\t\t\tself.previous_action = 0\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.previous_action = 1\n",
    "\t\t\n",
    "\t\treturn self.previous_action\n",
    "\n",
    "\tdef record(self,reward):\n",
    "\t\t# set reward to the last transition\n",
    "\t\tself.moves[-1][3] = reward\n",
    "\n",
    "\tdef update_qvalues(self, score):\n",
    "\t\tself.episode += 1\n",
    "\t\tself.max_score = max(self.max_score, score)\n",
    "\t\tprint(\"Episode: \" + str(self.episode) + \" Epsilon: \" + str(self.epsilon) + \" Score: \" + str(score) + \" Max Score: \" + str(self.max_score))\n",
    "\t\tself.scores.append(score)\n",
    "\t\t\n",
    "\t\tif self.train:\n",
    "\t\t\thistory = list(reversed(self.moves))\n",
    "\t\t\tfirst = True\n",
    "\t\t\tsecond = True\n",
    "\t\t\tjump = True\n",
    "\t\t\tif history[0][1] < 69:\n",
    "\t\t\t\tjump = False\n",
    "\t\t\tfor move in history:\n",
    "\t\t\t\t[x,y,v] = move[0]\n",
    "\t\t\t\taction = move[1]\n",
    "\t\t\t\t[x1,y1,z1] = move[2]\n",
    "\t\t\t\treward = move[3]\n",
    "\t\t\t\t# penalize last 2 states before crash\n",
    "\t\t\t\tif first or second:\n",
    "\t\t\t\t\treward = -1\n",
    "\t\t\t\t\tif first:\n",
    "\t\t\t\t\t\tfirst = False\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tsecond = False\n",
    "\t\t\t\t# penalize last jump before crash\n",
    "\t\t\t\tif jump and action:\n",
    "\t\t\t\t\treward = -1\n",
    "\t\t\t\t\tjump = False\n",
    "\t\t\t\tself.qvalues[x,y,v,action] = (1- self.learning_rate) * (self.qvalues[x,y,v,action]) + (self.learning_rate) * ( reward + (self.discount_factor)*max(self.qvalues[x1,y1,z1,0],self.qvalues[x1,y1,z1,1]))\n",
    "\n",
    "\t\t\tself.moves = []\n",
    "\t\t\t# decay epsilon linearly\n",
    "\t\t\tif self.epsilon > self.final_epsilon:\n",
    "\t\t\t\tself.epsilon -= self.epsilon_decay\n",
    "\t\t\n",
    "\tdef save_model(self):\n",
    "\t\t# write the episode, epsilon, qvalues to qvalues.txt\n",
    "\t\tdata = str(self.episode) + \",\" + str(self.epsilon) + \"\\n\"\n",
    "\t\tfor x in range(self.xdim):\n",
    "\t\t\tfor y in range(self.ydim):\n",
    "\t\t\t\tfor v in range(self.vdim):\n",
    "\t\t\t\t\tfor a in range(2):\n",
    "\t\t\t\t\t\tdata += str(x) + \", \" + str(y) + \", \" + str(v) + \", \" + str(a) + \", \" + str(self.qvalues[x,y,v,a]) + \"\\n\"\n",
    "\t\tqfile = open(\"qvalues_greedy.txt\",\"w\")\n",
    "\t\tqfile.write(data)\n",
    "\t\tqfile.close()\n",
    "\t\t\n",
    "\t\t# append the scores to scores.txt\n",
    "\t\tdata1 = ''\n",
    "\t\tfor i in range(len(self.scores)):\n",
    "\t\t\tdata1 += str(self.scores[i]) + \"\\n\"\n",
    "\t\tsfile = open(\"scores_greedy.txt\",\"a+\")\n",
    "\t\tsfile.write(data1)\n",
    "\t\tsfile.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a77ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
